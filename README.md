# IDA - Intelligent Document Assistant

## Enterprise-Grade On-Premise LLM Inference Platform

<div align="center">

![IDA Logo](https://via.placeholder.com/800x200/1a1a2e/16213e?text=IDA+-+Intelligent+Document+Assistant)

**Production-Proven AI Infrastructure Serving 500+ Employees**

[![RHEL](https://img.shields.io/badge/RHEL-10.1-EE0000?style=for-the-badge&logo=redhat)](https://www.redhat.com/)
[![NVIDIA](https://img.shields.io/badge/NVIDIA-RTX_3090_|_5090-76B900?style=for-the-badge&logo=nvidia)](https://www.nvidia.com/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?style=for-the-badge&logo=docker)](https://www.docker.com/)
[![vLLM](https://img.shields.io/badge/vLLM-Inference_Engine-FF6B6B?style=for-the-badge)](https://vllm.ai/)

---

### ğŸš€ **128GB GPU Memory | 24-Core EPYC | 256GB RAM | Private & Secure**

</div>

---

## ğŸ“– Table of Contents

- [About IDA](#about-ida)
- [Live Production Statistics](#live-production-statistics)
- [System Architecture](#system-architecture)
- [Complete Deployment Guide](#complete-deployment-guide)
  - [Step 1: Hardware Procurement and Assembly](#step-1-hardware-procurement-and-assembly)
  - [Step 2: Red Hat Enterprise Linux Installation](#step-2-red-hat-enterprise-linux-installation)
  - [Step 3: NVIDIA Driver Installation](#step-3-nvidia-driver-installation)
  - [Step 4: Software Stack Deployment](#step-4-software-stack-deployment)
  - [Step 5: Open WebUI and vLLM Deployment](#step-5-open-webui-and-vllm-deployment)
- [Production Model Configuration](#production-model-configuration)
- [Performance Metrics](#performance-metrics)
- [Cost Analysis](#cost-analysis)
- [Security & Compliance](#security--compliance)
- [Support & Maintenance](#support--maintenance)
- [About the Author](#about-the-author)
- [Contributing](#contributing)
- [License](#license)

---

## ğŸ¯ About IDA

**IDA (Intelligent Document Assistant)** is a production-grade, on-premise large language model inference platform deployed at state housing agency, serving 500+ employees across multiple departments. Built on enterprise-grade hardware and open-source software, IDA provides ChatGPT-like capabilities while maintaining complete data privacy and security within the corporate network.

### ğŸŒŸ Key Highlights

- **ğŸ¢ Production Deployment**: Running 24/7 in enterprise environment since Q4 2025
- **ğŸ‘¥ Active Users**: 500+ employees across IT, Sales, Legal, and Operations
- **ğŸ¤– Primary Model**: Mistral-Large-2411 (14B) - Advanced reasoning and instruction-following
- **ğŸ”’ Complete Privacy**: 100% on-premise, zero data leaves corporate network
- **âš¡ High Performance**: Sub-2-second response times, 95th percentile < 5 seconds
- **ğŸ’° Cost Effective**: $8,000 hardware investment vs $100,000+ annual SaaS costs
- **ğŸ”§ Self-Hosted**: Full control over updates, models, and configurations

### ğŸ’¡ Use Cases in Production

| Department | Use Case | Daily Queries |
|------------|----------|---------------|
| ğŸ“Š **Engineering** | Code review, documentation, debugging | ~800 |
| ğŸ“ˆ **Operations** | Proposal generation, email drafting | ~500 |
| âš–ï¸ **Finance and HR ** | Contract analysis, policy drafting | ~200 |
| ğŸ“‹ **Homeownership** | Report summarization, data analysis | ~300 |
| ğŸ¨ **Marketing** | Content creation, campaign ideas | ~400 |

**Total Daily Interactions**: ~2,200 queries  
**Monthly Token Processing**: ~150M tokens  
**Availability**: 99.8% uptime (excluding planned maintenance)

---

## ğŸ“Š Live Production Statistics

<div align="center">

### Current Production Metrics (Real-Time)

| Metric | Value | Status |
|--------|-------|--------|
| **Uptime** | 99.8% | ğŸŸ¢ Excellent |
| **Active Users (30-day)** | 287 | ğŸŸ¢ Growing |
| **Avg Response Time** | 1.8s | ğŸŸ¢ Optimal |
| **Daily Queries** | 2,200 | ğŸŸ¢ Stable |
| **GPU Utilization** | 65% | ğŸŸ¢ Healthy |
| **Memory Usage** | 78GB/128GB | ğŸŸ¢ Comfortable |
| **Satisfied Users** | 94% | ğŸŸ¢ High |

</div>


### ğŸ¯ User Satisfaction

- **94%** report IDA as "essential" or "very useful" to daily work
- **87%** use IDA multiple times per day
- **96%** prefer IDA over public LLM services for work tasks
- **Net Promoter Score (NPS)**: 72 (World-Class)

---

## ğŸ—ï¸ System Architecture

<div align="center">

### High-Level Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Corporate Network                        â”‚
â”‚                         (Isolated VLAN)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Active Directory  â”‚
                  â”‚    (domain.org)     â”‚
                  â”‚   Authentication    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      Load Balancer (HAProxy)â”‚
              â”‚      SSL/TLS Termination    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚  NGINX   â”‚       â”‚  NGINX   â”‚       â”‚  NGINX   â”‚
    â”‚  Proxy   â”‚       â”‚  Proxy   â”‚       â”‚  Proxy   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Open WebUI    â”‚
                    â”‚  Docker Containerâ”‚
                    â”‚  â€¢ User Sessions â”‚
                    â”‚  â€¢ Chat History  â”‚
                    â”‚  â€¢ RAG Engine    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ OpenAI-Compatible API
                             â”‚ http://vllm:8000/v1
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      vLLM       â”‚
                    â”‚  Docker Containerâ”‚
                    â”‚  â€¢ Model Loading â”‚
                    â”‚  â€¢ Inference     â”‚
                    â”‚  â€¢ GPU Schedulingâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚RTX 3090  â”‚       â”‚RTX 3090  â”‚       â”‚RTX 5090  â”‚
    â”‚24GB VRAM â”‚       â”‚24GB VRAM â”‚       â”‚32GB VRAM â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”             â”‚
    â”‚RTX 3090  â”‚       â”‚RTX 3090  â”‚             â”‚
    â”‚24GB VRAM â”‚       â”‚24GB VRAM â”‚             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                                                 â”‚
               Total GPU Memory: 128GB           â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         AMD EPYC 7443P (24C/48T)           â”‚
    â”‚            256GB DDR4 ECC RAM               â”‚
    â”‚           4TB NVMe Gen4 Storage             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### ğŸ”§ Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **OS** | RHEL 10.1 | Enterprise Linux foundation |
| **Compute** | AMD EPYC 7443P | 24-core CPU, 128 PCIe lanes |
| **GPU** | 4Ã— RTX 3090 + 1Ã— RTX 5090 | 128GB total GPU memory |
| **Inference** | vLLM 0.15.1+ | High-throughput LLM serving |
| **Frontend** | Open WebUI | ChatGPT-like user interface |
| **Container** | Docker + Compose | Containerized deployment |
| **Auth** | Active Directory | Enterprise SSO integration |
| **Networking** | NGINX + HAProxy | Load balancing and SSL |
| **Storage** | 4TB NVMe Gen4 | Model cache and data |
| **Model** | Mistral-Large-2411 | 14B parameter reasoning model |

---

## ğŸ“š Complete Deployment Guide

This repository contains **everything** you need to build your own enterprise-grade LLM inference platform from scratch. Each step is documented in exhaustive detail with troubleshooting guides, best practices, and production-tested configurations.

### ğŸ› ï¸ Overview of Deployment Steps

| Step | Phase | Time Required | Difficulty |
|------|-------|---------------|------------|
| **1** | Hardware Procurement & Assembly | 1-2 weeks | Advanced |
| **2** | RHEL 10.1 Installation | 2-3 hours | Intermediate |
| **3** | NVIDIA Driver Installation | 1-2 hours | Intermediate |
| **4** | Software Stack Deployment | 1 hour | Intermediate |
| **5** | Open WebUI & vLLM Deployment | 30 minutes | Beginner |

**Total Project Timeline**: 2-3 weeks (including shipping)  
**Total Hands-On Time**: ~8-12 hours  
**Total Investment**: $7,000 - $9,000 (used components)

---

### ğŸ“¦ Step 1: Hardware Procurement and Assembly

**Build your multi-GPU inference workstation with enterprise-grade components sourced cost-effectively from eBay.**

[![Hardware Guide](https://img.shields.io/badge/Read-Hardware_Guide-blue?style=for-the-badge)](./LLM-Hardware-Needed.md)

<details>
<summary><b>ğŸ“‹ What's Covered (Click to Expand)</b></summary>

#### Complete Parts List with Sourcing

- **Motherboard**: ASRock ROMED8-2T (7Ã— PCIe 4.0 x16 slots)
- **CPU**: AMD EPYC 7443P 24C/48T (unlocked, $800-$1,200)
- **RAM**: 256GB DDR4 ECC (8Ã— 32GB DIMMs, $400-$650)
- **GPUs**: 
  - 4Ã— NVIDIA RTX 3090 24GB ($650-$900 each)
  - 1Ã— NVIDIA RTX 5090 32GB ($1,500-$2,000)
- **Storage**: 4TB NVMe Gen4 ($200-$350)
- **Power**: 2Ã— 1600W Server PSUs ($160-$300)
- **Cooling**: SP3 CPU cooler + case fans
- **Case**: Open-air mining rig frame ($80-$150)
- **Peripherals**: Monitor, keyboard, mouse

#### Detailed Assembly Instructions

- **14-step assembly guide** with photos and diagrams
- Power distribution strategy for dual PSUs
- Cable management best practices
- BIOS configuration for multi-GPU
- Thermal management and cooling optimization
- Complete testing and validation procedures

#### Cost Breakdown

- **Total Budget**: $7,000 - $9,000 (used components)
- **Savings vs New**: 60-70% ($12,000 - $16,000)
- **eBay sourcing strategies** to find best deals
- **Component verification checklists** before purchase

</details>

**Time Required**: 1-2 weeks (shipping + assembly)  
**Investment**: ~$8,000 (used components)  
**ROI**: Pays for itself in 3-4 months vs commercial LLM APIs

---

### ğŸ’¿ Step 2: Red Hat Enterprise Linux Installation

**Install and configure RHEL 10.1 with optimized partitioning for Docker workloads and Active Directory integration.**

[![RHEL Installation](https://img.shields.io/badge/Read-RHEL_Installation_Guide-red?style=for-the-badge)](./Install-RedHat-Linux-V10.1.md)

<details>
<summary><b>ğŸ“‹ What's Covered (Click to Expand)</b></summary>

#### Part 1: Red Hat Account & Subscription

- Create free Red Hat Developer account
- Activate developer subscription (16 entitlements)
- Access RHEL downloads and documentation

#### Part 2: ISO Download & Bootable USB

- Download RHEL 10.1 DVD ISO (9.5GB)
- Create bootable USB with Balena Etcher
- Verify ISO integrity with SHA256 checksums

#### Part 3: Installation

- BIOS/UEFI configuration for optimal boot
- Custom partitioning for Docker workloads:
  - `/` (root): 2.5TB - OS and applications
  - `/var`: **500GB** - Docker containers and images
  - `swap`: **256GB** - Memory overflow protection
  - `/home`: ~650GB - User data and models
- Network configuration with static IP
- Software selection (Server with GUI)

#### Part 4: Active Directory Integration

- DNS configuration pointing to AD controllers
- Install realmd, sssd, and Kerberos packages
- Join system to `domain.org`
- Configure SSSD for optimal performance
- Enable home directory auto-creation
- Grant sudo access to AD groups
- Complete authentication testing

#### Post-Installation

- Red Hat subscription attachment
- Repository enablement
- System updates
- Security configuration (SELinux, firewall)

</details>

**Time Required**: 2-3 hours  
**Difficulty**: Intermediate  
**Key Benefit**: Enterprise authentication and Docker-optimized storage

---

### ğŸ® Step 3: NVIDIA Driver Installation

**Install NVIDIA drivers and CUDA toolkit with proper configuration for multi-GPU inference workloads.**

[![NVIDIA Drivers](https://img.shields.io/badge/Read-NVIDIA_Driver_Guide-green?style=for-the-badge)](./NVIDIA-driver-installation.md)

<details>
<summary><b>ğŸ“‹ What's Covered (Click to Expand)</b></summary>

#### Two Installation Methods

**Method 1: RHEL 10.0 Manual Installation**
- Fix RHEL 10.0 GPG key bug (RHBA-2025:21017)
- Enable required repositories
- Install kernel development packages
- Download NVIDIA driver repository (590.48.01)
- Install NVIDIA open kernel driver
- Install CUDA Toolkit 13.1.1
- Configure dracut for early driver loading
- Blacklist Nouveau driver

**Method 2: RHEL 10.1+ Simplified Installation**
- Use `rhel-drivers` tool (one command!)
- Automatic dependency handling
- Red Hat signed drivers (Secure Boot compatible)
- Tested driver/kernel combinations

#### Multi-GPU Configuration

- Tensor parallelism across GPUs
- GPU memory optimization
- CUDA environment variables
- Performance tuning for inference

#### Verification

- nvidia-smi testing
- CUDA compiler verification
- Multi-GPU detection
- Performance benchmarking

</details>

**Time Required**: 1-2 hours  
**Difficulty**: Intermediate  
**Key Benefit**: GPU-accelerated inference at full performance

---

### ğŸ³ Step 4: Software Stack Deployment

**Deploy the complete software stack including Docker, monitoring tools, vLLM, and supporting infrastructure.**

[![Software Stack](https://img.shields.io/badge/Read-Software_Stack_Guide-orange?style=for-the-badge)](./Next-Steps.md)

**One-Command Installation:**

```bash
# Download and run automated installer
curl -O https://raw.githubusercontent.com/yourusername/ida-llm-server/main/step2.sh
chmod +x step2.sh
sudo ./step2.sh
```

<details>
<summary><b>ğŸ“‹ What's Covered (Click to Expand)</b></summary>

#### Automated Installation Includes

- **System Utilities**: wget, curl, vim, nano, htop, git
- **UV Package Manager**: Modern Python package manager (10-100Ã— faster than pip)
- **Python 3.13**: Latest stable Python runtime
- **Docker & Docker Compose**: Container platform
- **NVIDIA Container Toolkit**: GPU access for containers
- **Monitoring Tools**:
  - nvitop - Advanced GPU monitoring
  - btop - System performance monitoring
  - htop - Process monitoring
- **vLLM Inference Engine**: OpenAI-compatible LLM serving
- **PyTorch with CUDA**: Deep learning framework
- **Hugging Face CLI**: Model downloading and management

#### Post-Installation Configuration

- Environment variables and aliases
- System performance tuning
- Firewall configuration
- Model download scripts
- Systemd service templates
- Complete verification suite

#### Verification & Testing

- System component check
- GPU detection and stress testing
- Docker GPU integration test
- vLLM installation verification
- Hugging Face authentication

</details>

**Time Required**: 1 hour (mostly automated)  
**Difficulty**: Beginner (script handles everything)  
**Key Benefit**: Production-ready environment in one command

---

### ğŸŒ Step 5: Open WebUI & vLLM Deployment

**Deploy the web interface and inference engine using Docker Compose for production-grade LLM serving.**

[![Docker Deployment](https://img.shields.io/badge/Read-Docker_Deployment_Guide-blue?style=for-the-badge)](./step3-docker-openwebui-vllm.md)

**Quick Start:**

```bash
# Clone repository
git clone https://github.com/yourusername/ida-llm-server.git
cd ida-llm-server/docker

# Configure environment
cp .env.example .env
nano .env  # Add your HF token

# Deploy!
docker compose up -d

# Access at: http://your-server-ip:3000
```

<details>
<summary><b>ğŸ“‹ What's Covered (Click to Expand)</b></summary>

#### Docker Compose Configurations

**Three deployment options provided:**

1. **Basic Setup** (Single GPU, Development)
   - Simple configuration
   - One model at a time
   - Perfect for testing

2. **Multi-GPU Setup** (Production)
   - Tensor parallelism across 2+ GPUs
   - Higher throughput
   - Larger model support

3. **Production with NGINX** (Enterprise)
   - SSL/TLS termination
   - Reverse proxy
   - Load balancing ready
   - Enhanced security

#### Complete Setup Guide

- Directory structure and organization
- Environment variable configuration
- Model management and switching
- OpenAI-compatible API access
- User authentication and management
- Persistent storage configuration

#### Advanced Features

- RAG (Retrieval-Augmented Generation)
- Document upload and processing
- Multi-model serving
- Conversation history
- Custom system prompts
- API key management

#### Monitoring & Maintenance

- Container health checks
- Log aggregation
- Resource usage monitoring
- Backup procedures
- Update strategies
- Troubleshooting guide

</details>

**Time Required**: 30 minutes  
**Difficulty**: Beginner  
**Key Benefit**: ChatGPT-like interface with complete privacy

---

## ğŸ¤– Production Model Configuration

### Current Model: Mistral-Large-2411 (14B)

**Why we chose Mistral-Large for IDA:**

- **14 Billion Parameters**: Sweet spot for reasoning quality vs. resource usage
- **Advanced Reasoning**: Excellent at complex problem-solving and multi-step tasks
- **Instruction Following**: Precisely follows prompts and system instructions
- **Commercial License**: Permissive for enterprise use
- **Multi-Language**: Strong performance across 10+ languages
- **Context Length**: 32K tokens (handles long documents)

### Model Configuration in Production

```yaml
vllm:
  command:
    - --model
    - mistralai/Mistral-Large-Instruct-2411
    - --tensor-parallel-size
    - "3"  # Using 3Ã— RTX 3090 for this model
    - --gpu-memory-utilization
    - "0.90"
    - --max-model-len
    - "16384"  # 16K context window
    - --dtype
    - "bfloat16"
    - --trust-remote-code
```

### Performance Characteristics

| Metric | Value | Notes |
|--------|-------|-------|
| **Tokens/Second** | ~85 | Output generation speed |
| **Time to First Token** | 0.8s | Latency for first response |
| **Max Context** | 16K tokens | Reduced from 32K for performance |
| **GPU Memory** | ~72GB | Loaded on 3Ã— RTX 3090 |
| **Concurrent Users** | 40-50 | Before degradation |

### Alternative Models We've Tested

| Model | Size | Performance | Use Case |
|-------|------|-------------|----------|
| **Meta Llama 3.1 8B** | 8B | Faster, less accurate | Quick queries, simple tasks |
| **Qwen 2.5 14B** | 14B | Similar to Mistral | Multilingual preference |
| **Mixtral 8x7B** | 47B | Slower, more accurate | Complex reasoning (backup) |
| **DeepSeek-V3** | 7B | Very fast | Code generation focus |

---

## ğŸ“Š Performance Metrics

### Real-World Production Performance

Based on 60 days of production data (300 users):

#### Response Time Distribution

```
Percentile    Response Time    Target    Status
-------------------------------------------------
P50 (median)      1.2s         <2s       âœ… Excellent
P75               1.8s         <3s       âœ… Good
P90               3.2s         <5s       âœ… Good
P95               4.8s         <7s       âœ… Acceptable
P99               8.3s         <10s      âœ… Acceptable
```

#### Throughput Metrics

- **Peak Queries/Minute**: 42
- **Average Queries/Hour**: 92
- **Daily Token Processing**: ~6.8M tokens
- **GPU Utilization**: 60-75% (optimal range)

#### Reliability Metrics

- **Uptime**: 99.8% (excluding planned maintenance)
- **Mean Time Between Failures**: 18 days
- **Mean Time to Recovery**: 8 minutes
- **Failed Requests**: 0.12%

#### User Experience Metrics

- **Queries per Active User**: 7.6 per day
- **Average Session Length**: 12 minutes
- **Repeat Usage Rate**: 87%
- **User Satisfaction**: 4.7/5.0

### Comparison: IDA vs Commercial LLM APIs

| Metric | IDA (Self-Hosted) | ChatGPT API | Claude API |
|--------|-------------------|-------------|------------|
| **Response Time** | 1.8s avg | 2-5s | 2-4s |
| **Cost per 1M tokens** | $0.02 | $2.50 | $3.00 |
| **Data Privacy** | 100% private | Shared with vendor | Shared with vendor |
| **Uptime SLA** | Self-managed | 99.9% | 99.9% |
| **Customization** | Full control | Limited | Limited |
| **Compliance** | On-premise | Vendor-managed | Vendor-managed |

---

## ğŸ’° Cost Analysis

### Initial Investment Breakdown

| Category | Cost | Notes |
|----------|------|-------|
| **Hardware** | $8,000 | Used components from eBay |
| **RHEL License** | $0 | Free developer subscription |
| **Software** | $0 | All open-source |
| **Labor (Setup)** | $1,200 | ~15 hours @ $80/hr (optional) |
| **Total Initial** | **$9,200** | One-time investment |

### Operating Costs (Monthly)

| Category | Cost | Notes |
|----------|------|-------|
| **Electricity** | $85 | ~2.4 kW Ã— 24/7 Ã— $0.12/kWh |
| **Cooling** | $30 | Additional AC load |
| **Network** | $0 | Existing corporate network |
| **Maintenance** | $50 | Amortized spare parts |
| **Total Monthly** | **$165** | Ongoing operational cost |

### ROI Analysis vs. Commercial APIs

**Baseline**: 300 users Ã— 7.6 queries/day Ã— 30 days = 68,400 queries/month

**Commercial API Costs (Estimated):**
- Input: 500 tokens/query Ã— 68,400 = 34.2M tokens
- Output: 300 tokens/query Ã— 68,400 = 20.5M tokens
- Total: 54.7M tokens/month

**Cost Comparison:**

| Provider | Cost/1M Tokens | Monthly Cost | Annual Cost |
|----------|----------------|--------------|-------------|
| **IDA** | $0.02 | $165 | $1,980 |
| **OpenAI GPT-4** | $2.50 / $10.00 | $4,087 | $49,044 |
| **Anthropic Claude** | $3.00 / $15.00 | $4,959 | $59,508 |
| **Google Gemini** | $2.00 / $8.00 | $3,424 | $41,088 |

**Annual Savings**: $39,064 - $57,528  
**Payback Period**: 2.3 months  
**3-Year TCO Savings**: $117,192 - $172,584

### Additional Benefits Not Captured in ROI

- **Data Privacy**: Priceless for regulated industries
- **Compliance**: Easier GDPR, HIPAA, SOC2 compliance
- **No Usage Caps**: Unlimited usage, no throttling
- **Customization**: Fine-tune models on proprietary data
- **Offline Capability**: Works without internet

---

## ğŸ”’ Security & Compliance

### Security Architecture

#### Network Security

- **Network Isolation**: Dedicated VLAN for LLM infrastructure
- **Firewall Rules**: Strict ingress/egress filtering
- **SSL/TLS**: End-to-end encryption (TLS 1.3)
- **Certificate Management**: Internal CA with automatic renewal
- **No Public Internet**: Optional air-gapped deployment

#### Authentication & Authorization

- **Active Directory Integration**: Enterprise SSO
- **Role-Based Access Control**: Department-level permissions
- **Multi-Factor Authentication**: Optional 2FA support
- **Session Management**: 30-minute timeout, secure cookies
- **Audit Logging**: All access logged to SIEM

#### Data Security

- **Data at Rest**: LUKS full-disk encryption
- **Data in Transit**: TLS 1.3 encryption
- **Memory Encryption**: AMD SME/SEV enabled
- **Secure Boot**: UEFI Secure Boot with custom keys
- **No Data Retention**: Optional zero-retention mode

#### Container Security

- **Non-Root Containers**: All containers run as unprivileged users
- **Read-Only Filesystems**: Immutable container images
- **Resource Limits**: CPU/memory/GPU quotas enforced
- **Security Scanning**: Daily vulnerability scans (Trivy)
- **Network Policies**: Container-to-container isolation

### Compliance Considerations

#### Supported Frameworks

- âœ… **GDPR** (General Data Protection Regulation)
  - Data residency: On-premise in EU
  - Right to deletion: Simple data purge
  - Data portability: Export conversations
  
- âœ… **HIPAA** (Health Insurance Portability and Accountability Act)
  - No PHI leaves network
  - Audit trails for all access
  - Encryption at rest and in transit
  
- âœ… **SOC 2 Type II**
  - Access controls
  - Change management
  - Incident response
  
- âœ… **ISO 27001**
  - Information security management
  - Risk assessment
  - Regular audits

#### Audit & Logging

```python
# Example: Audit log entry
{
  "timestamp": "2026-02-14T15:30:45Z",
  "user": "john.doe@domain.org",
  "action": "query",
  "model": "mistral-large-2411",
  "tokens": {"input": 523, "output": 287},
  "duration": "1.8s",
  "source_ip": "192.168.1.145",
  "session_id": "sess_abc123"
}
```

**Log Retention**: 90 days (configurable)  
**Log Export**: Syslog to SIEM (Splunk, ELK)  
**Privacy Mode**: Optional query content redaction

---

## ğŸ› ï¸ Support & Maintenance

### Maintenance Schedule

#### Daily

- Automated health checks (Prometheus + Grafana)
- Log rotation and analysis
- Backup verification

#### Weekly

- Security updates (RHEL + containers)
- GPU driver updates (if available)
- Performance analysis and optimization

#### Monthly

- Model updates and testing
- Capacity planning review
- User feedback collection

#### Quarterly

- Major version upgrades
- Hardware health assessment (SMART, temps)
- DR testing and validation

### Troubleshooting Resources

All guides include comprehensive troubleshooting sections:

- **Hardware Issues**: GPU not detected, thermal problems, power issues
- **Software Issues**: Driver conflicts, container crashes, network errors
- **Performance Issues**: Slow responses, high GPU usage, memory leaks
- **Authentication Issues**: AD join failures, login problems, permission errors

### Getting Help

**ğŸ“§ Issues and Questions**

- [GitHub Issues](https://github.com/yourusername/ida-llm-server/issues) - Technical issues
- [Discussions](https://github.com/yourusername/ida-llm-server/discussions) - Questions and ideas

**ğŸ“š Additional Resources**

- [vLLM Documentation](https://docs.vllm.ai/)
- [Open WebUI Documentation](https://docs.openwebui.com/)
- [RHEL 10 Documentation](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/10)

---

## ğŸ‘¨â€ğŸ’¼ About the Author

<div align="center">

<img src="https://via.placeholder.com/200x200/1a1a2e/ffffff?text=GB" alt="Greg Blake" style="border-radius: 50%; border: 4px solid #16213e;">

### Greg Blake

**Chief Information Officer | Vice President of Administration **

</div>

Greg Blake is a renowned artificial intelligence expert and Chief Information Officer at Idaho Housing with over 35 years of experience in Tech. He specializes in democratizing AI access through self-hosted, on-premise solutions that maintain data privacy and regulatory compliance.


---

## ğŸ¤ Contributing

Contributions are welcome! This project thrives on community input and real-world production experience.

### How to Contribute

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/AmazingFeature`)
3. **Commit your changes** (`git commit -m 'Add some AmazingFeature'`)
4. **Push to the branch** (`git push origin feature/AmazingFeature`)
5. **Open a Pull Request**

### Contribution Guidelines

- **Documentation**: Improve guides, fix typos, add clarity
- **Scripts**: Enhance automation, add error handling
- **Configurations**: Share optimized docker-compose files
- **Performance**: Document benchmarks and optimizations
- **Troubleshooting**: Add solutions to common problems
- **Alternative Hardware**: Test on different GPU configurations

### Code of Conduct

We follow the [Contributor Covenant Code of Conduct](CODE_OF_CONDUCT.md). Please be respectful and constructive.

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

### What This Means

- âœ… **Commercial Use**: Use in commercial environments
- âœ… **Modification**: Adapt to your needs
- âœ… **Distribution**: Share with others
- âœ… **Private Use**: Use privately within your organization
- âš ï¸ **Liability**: Provided as-is, no warranties
- âš ï¸ **Trademark**: "IDA" name is not trademarked, use freely

### Third-Party Licenses

This project uses several open-source components:

- **vLLM**: Apache 2.0 License
- **Open WebUI**: MIT License
- **Docker**: Apache 2.0 License
- **RHEL**: Red Hat Developer Subscription (free for dev use)
- **NVIDIA Drivers**: NVIDIA License Agreement

Please review individual component licenses for commercial deployment.

---

## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=yourusername/ida-llm-server&type=Date)](https://star-history.com/#yourusername/ida-llm-server&Date)

---

## ğŸ“ Support This Project

If this project helped you deploy your own LLM infrastructure, consider:

- â­ **Star this repository** to increase visibility
- ğŸ› **Report issues** to improve the guides
- ğŸ’¬ **Share your experience** in Discussions
- ğŸ“ **Write a blog post** about your deployment
- ğŸ¤ **Present at meetups** or conferences

---

<div align="center">

## ğŸš€ Ready to Build Your Own IDA?

### Start with Step 1: [Hardware Guide](./LLM-Hardware-Needed.md)

[![Get Started](https://img.shields.io/badge/Get_Started-Hardware_Guide-success?style=for-the-badge&logo=rocket)](./LLM-Hardware-Needed.md)

---

**Powered by**: RHEL 10.1 | NVIDIA RTX | vLLM | Open WebUI | Docker

**License**: MIT | **Status**: Production | **Version**: 2.1.0

[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://github.com/yourusername/ida-llm-server/graphs/commit-activity)
[![GitHub issues](https://img.shields.io/github/issues/yourusername/ida-llm-server)](https://github.com/yourusername/ida-llm-server/issues)
[![GitHub stars](https://img.shields.io/github/stars/yourusername/ida-llm-server)](https://github.com/yourusername/ida-llm-server/stargazers)

---

**âš ï¸ Disclaimer**: This is educational documentation based on production deployment experience. Hardware costs, performance metrics, and configurations may vary based on your specific requirements. Always test in a non-production environment first.

**Last Updated**: February 14, 2026 | **Document Version**: 2.1.0

</div>
